import gymnasium as gym
from gymnasium import spaces
import numpy as np
import torch
import yaml
import re
from AcSimulation import AcSimulationCuDSS, AcSimulationPardiso
from build_ckt import build_ac_ckt
from Circuit import Circuit, BranchType

import torch.nn as nn
import torch.nn.functional as F

from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

# class CustomCombinedExtractor(BaseFeaturesExtractor):
#     def __init__(self, observation_space: gym.spaces.Dict, hidden_dim: int = 256):
#         # features_dim éœ€è¦æ‰‹åŠ¨è®¡ç®—ï¼Œç­‰äºæ‰€æœ‰éƒ¨åˆ†ç‰¹å¾æ‹¼æ¥åçš„æ€»ç»´åº¦
#         # æˆ‘ä»¬è®©çº¿æ€§éƒ¨åˆ†çš„è¾“å‡ºç»´åº¦å’ŒCNNéƒ¨åˆ†çš„è¾“å‡ºç»´åº¦éƒ½ä¸º cnn_output_dim
#         features_dim = hidden_dim * 3 # impedance + target_z + decap_map
#         super().__init__(observation_space, features_dim)

#         # æå–æ¯ä¸ªè¾“å…¥çš„ç»´åº¦ä¿¡æ¯
#         impedance_dim = observation_space["impedance"].shape[0]
#         target_z_dim = observation_space["target_z"].shape[0]
#         decap_map_shape = observation_space["decap_map"].shape # (width, height)
#         decap_map_dim_flat = decap_map_shape[0] * decap_map_shape[1]
#         # æˆ‘ä»¬å°†æ¯ä¸ªéƒ¨åˆ†éƒ½æ˜ å°„åˆ°æŒ‡å®šçš„ hidden_dim
#         self.impedance_fc = nn.Linear(impedance_dim, hidden_dim)
#         self.target_z_fc = nn.Linear(target_z_dim, hidden_dim)
#         self.decap_map_fc = nn.Linear(decap_map_dim_flat, hidden_dim)
        
#         # # --- ä¸º1Då‘é‡æ•°æ®ï¼ˆé˜»æŠ—ã€ç›®æ ‡ï¼‰åˆ›å»ºå¤„ç†æµæ°´çº¿ (å…¨è¿æ¥å±‚) ---
#         # self.vector_net = nn.Sequential(
#         #     nn.Linear(impedance_dim + target_z_dim, 256),
#         #     nn.ReLU(),
#         #     nn.Linear(256, cnn_output_dim) # è¾“å‡ºä¸€ä¸ª256ç»´çš„ç‰¹å¾
#         # )

#         # # --- ä¸º2Dç½‘æ ¼æ•°æ®ï¼ˆç”µå®¹åœ°å›¾ï¼‰åˆ›å»ºå¤„ç†æµæ°´çº¿ (å·ç§¯å±‚) ---
#         # # è¿™ä¸ªCNNç»“æ„å‚è€ƒäº†é€šç”¨å›¾åƒå¤„ç†ç½‘ç»œï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
#         # self.cnn = nn.Sequential(
#         #     # è¾“å…¥é€šé“ä¸º1ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç”µå®¹åœ°å›¾æ˜¯å•å±‚çš„
#         #     nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
#         #     nn.ReLU(),
#         #     nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # stride=2ä¼šç¼©å°å›¾åƒå°ºå¯¸
#         #     nn.ReLU(),
#         #     nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),
#         #     nn.ReLU(),
#         #     nn.Flatten(), # å°†å·ç§¯åçš„äºŒç»´ç‰¹å¾å›¾å±•å¹³
#         # )
        
#         # # è®¡ç®—CNNå±•å¹³åçš„è¾“å‡ºç»´åº¦
#         # # æˆ‘ä»¬éœ€è¦ç”¨ä¸€ä¸ªå‡çš„è¾“å…¥æ¥åŠ¨æ€è®¡ç®—è¿™ä¸ªç»´åº¦
#         # with torch.no_grad():
#         #     dummy_input = torch.as_tensor(np.zeros((1, 1) + decap_map_shape))
#         #     cnn_output_shape = self.cnn(dummy_input).shape[1]

#         # # å†æ¥ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå°†å±•å¹³åçš„ç‰¹å¾æ˜ å°„åˆ°æœ€ç»ˆçš„ç»´åº¦
#         # self.cnn_linear = nn.Sequential(
#         #     nn.Linear(cnn_output_shape, cnn_output_dim),
#         #     nn.ReLU()
#         # )
        
#         # # (ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å°† impedance, target_z, å’Œ decap_map åˆ†å¼€å¤„ç†)
#         # # è®ºæ–‡ä¸­æ˜¯å°†æ‰€æœ‰ä¸œè¥¿éƒ½å¤„ç†æˆ16x16å†ç”¨å·ç§¯ï¼Œè¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªç®€åŒ–ç‰ˆï¼Œæ•ˆæœé€šå¸¸ä¹Ÿå¾ˆå¥½
#         # # ä¸‹é¢æ˜¯ä¸€ä¸ªæ›´ç¬¦åˆæ‚¨ä¹‹å‰æ€è·¯çš„ã€æ›´ç®€å•çš„å…¨è¿æ¥ç‰ˆæœ¬
#         # self.impedance_fc = nn.Linear(impedance_dim, 128)
#         # self.target_z_fc = nn.Linear(target_z_dim, 128)
#         # self.decap_map_fc = nn.Linear(decap_map_shape[0] * decap_map_shape[1], 128) # å°†äºŒç»´åœ°å›¾å±•å¹³åå¤„ç†
        
#         # # é‡æ–°è®¡ç®—æ€»ç‰¹å¾ç»´åº¦
#         # self._features_dim = 128 + 128 + 128
        
#     def forward(self, observations: dict) -> torch.Tensor:
#         impedance = observations["impedance"]
#         target_z = observations["target_z"]
#         decap_map = observations["decap_map"]

#         decap_map_flat = torch.flatten(decap_map, start_dim=1)

#         impedance_features = F.relu(self.impedance_fc(impedance))
#         target_z_features = F.relu(self.target_z_fc(target_z))
#         decap_map_features = F.relu(self.decap_map_fc(decap_map_flat))
        
#         concatenated_features = torch.cat([impedance_features, target_z_features, decap_map_features], dim=1)
        
#         return concatenated_features

## æ—§ç‰ˆæœ¬
# class CustomCombinedExtractor(BaseFeaturesExtractor):
#     def __init__(self, observation_space, hidden_dim=256):
#         super().__init__(observation_space, self._features_dim)

#         # 1. æå–ç»´åº¦ä¿¡æ¯
#         impedance_dim = observation_space["impedance"].shape[0]
#         target_z_dim = observation_space["target_z"].shape[0]
#         decap_shape = observation_space["decap_map"].shape  # (W, H)
#         context_shape = observation_space["context_map"].shape  # (C, W, H)

#         self.hidden_dim = hidden_dim

#         # 2. å…¨è¿æ¥ï¼šé˜»æŠ—å’Œç›®æ ‡é˜»æŠ—
#         self.impedance_fc = nn.Sequential(
#             nn.Linear(impedance_dim, hidden_dim),
#             nn.ReLU()
#         )
#         self.target_z_fc = nn.Sequential(
#             nn.Linear(target_z_dim, hidden_dim),
#             nn.ReLU()
#         )

#         # 3. CNN: decap_map (1 channel)
#         self.decap_cnn = nn.Sequential(
#             nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Flatten(),
#         )

#         # 4. CNN: context_map (å¤šé€šé“)
#         self.context_cnn = nn.Sequential(
#             nn.Conv2d(context_shape[0], 32, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Flatten(),
#         )

#         # 5. åŠ¨æ€è®¡ç®— Flatten åç»´åº¦
#         with torch.no_grad():
#             dummy_decap = torch.zeros(1, 1, *decap_shape)  # (B, 1, W, H)
#             decap_feat_dim = self.decap_cnn(dummy_decap).shape[1]

#             dummy_context = torch.zeros(1, *context_shape)  # (B, C, W, H)
#             context_feat_dim = self.context_cnn(dummy_context).shape[1]

#         # 6. æœ€ç»ˆçº¿æ€§å±‚
#         total_input_dim = hidden_dim * 2 + decap_feat_dim + context_feat_dim
#         self.linear = nn.Sequential(
#             nn.Linear(total_input_dim, hidden_dim * 2),
#             nn.ReLU(),
#             nn.Linear(hidden_dim * 2, hidden_dim),
#             nn.ReLU()
#         )

#         self._features_dim = hidden_dim  # é‡è¦ï¼šä¾› SB3 ä½¿ç”¨

#     def forward(self, obs):
#         # obs æ˜¯ä¸€ä¸ªå­—å…¸
#         impedance = obs["impedance"]
#         target_z = obs["target_z"]
#         decap_map = obs["decap_map"].unsqueeze(1)  # (B, 1, H, W)
#         context_map = obs["context_map"]  # (B, C, H, W)

#         # 1. å„éƒ¨åˆ†æå–ç‰¹å¾
#         imp_feat = self.impedance_fc(impedance)
#         targ_feat = self.target_z_fc(target_z)
#         decap_feat = self.decap_cnn(decap_map)
#         context_feat = self.context_cnn(context_map)

#         # 2. æ‹¼æ¥æ‰€æœ‰
#         concat = torch.cat([imp_feat, targ_feat, decap_feat, context_feat], dim=1)
#         return self.linear(concat)


class CustomCombinedExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space, hidden_dim=256):
        # ä¸´æ—¶è®¾ç½®å ä½å€¼ï¼Œç¨åä¼šæ­£ç¡®èµ‹å€¼ _features_dim
        super().__init__(observation_space, features_dim=1)

        self.hidden_dim = hidden_dim

        # è·å–å„è¾“å…¥ç»´åº¦
        impedance_dim = observation_space["impedance"].shape[0]
        target_z_dim = observation_space["target_z"].shape[0]
        decap_shape = observation_space["decap_map"].shape  # (H, W)
        context_shape = observation_space["context_map"].shape  # (C, H, W)
        global_shape = observation_space["global_map"].shape    # (C_g, 64, 64)
        local_mask_shape = observation_space["local_mask"].shape  # (1, 64, 64)

        # --- 1. å…¨è¿æ¥ï¼šimpedance / target_z ---
        self.imp_fc = nn.Sequential(
            nn.Linear(impedance_dim, hidden_dim),
            nn.ReLU()
        )
        self.targ_fc = nn.Sequential(
            nn.Linear(target_z_dim, hidden_dim),
            nn.ReLU()
        )

        # --- 2. CNN åˆ†æ”¯ï¼šdecap_map (B, 1, H, W) ---
        self.decap_cnn = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten()
        )

        # --- 3. CNN åˆ†æ”¯ï¼šcontext_map (B, C, H, W) ---
        self.context_cnn = nn.Sequential(
            nn.Conv2d(context_shape[0], 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten()
        )

        # --- 4. CNN åˆ†æ”¯ï¼šglobal_map (B, C, 64, 64) ---
        self.global_cnn = nn.Sequential(
            nn.Conv2d(global_shape[0], 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Flatten()
        )

        # --- 5. CNN åˆ†æ”¯ï¼šlocal_mask (B, 1, 64, 64) ---
        self.mask_cnn = nn.Sequential(
            nn.Conv2d(1, 8, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(8, 8, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Flatten()
        )

        # --- 6. è®¡ç®— flatten åç»´åº¦ ---
        with torch.no_grad():
            dummy_decap = torch.zeros(1, 1, *decap_shape)
            dummy_context = torch.zeros(1, *context_shape)
            dummy_global = torch.zeros(1, *global_shape)
            dummy_mask = torch.zeros(1, *local_mask_shape)

            decap_feat_dim = self.decap_cnn(dummy_decap).shape[1]
            context_feat_dim = self.context_cnn(dummy_context).shape[1]
            global_feat_dim = self.global_cnn(dummy_global).shape[1]
            mask_feat_dim = self.mask_cnn(dummy_mask).shape[1]

        # --- 7. æ•´åˆæ‰€æœ‰ç‰¹å¾ ---
        total_dim = hidden_dim * 2 + decap_feat_dim + context_feat_dim + global_feat_dim + mask_feat_dim
        self.linear = nn.Sequential(
            nn.Linear(total_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU()
        )

        self._features_dim = hidden_dim  # æ›´æ–°ä¸º SB3 æ‰€éœ€çš„ç‰¹å¾ç»´åº¦

    def forward(self, obs):
        # 1. å…¨è¿æ¥è¾“å…¥
        imp_feat = self.imp_fc(obs["impedance"])
        targ_feat = self.targ_fc(obs["target_z"])

        # 2. å·ç§¯è¾“å…¥
        decap_feat = self.decap_cnn(obs["decap_map"].unsqueeze(1))      # (B, 1, H, W)
        context_feat = self.context_cnn(obs["context_map"])             # (B, C, H, W)
        global_feat = self.global_cnn(obs["global_map"])                # (B, C_g, 64, 64)
        mask_feat = self.mask_cnn(obs["local_mask"])                    # (B, 1, 64, 64)

        # 3. æ‹¼æ¥å…¨éƒ¨ç‰¹å¾
        concat = torch.cat([imp_feat, targ_feat, decap_feat, context_feat, global_feat, mask_feat], dim=1)
        return self.linear(concat)


class CircuitEnv(gym.Env):
    metadata = {'render_modes': ['console']}

    def __init__(self, ckt_file, result_file, render_mode=None):
        super(CircuitEnv, self).__init__()

        # ------------------ 1. åˆå§‹åŒ–ç¯å¢ƒå‚æ•° (ä¸æ‚¨åŸç‰ˆç›¸åŒ) ------------------
        with open(ckt_file, "r") as f:
            self.design = yaml.load(f.read(), Loader=yaml.FullLoader)
        with open(result_file, "r") as f:
            self.initial_result = yaml.load(f.read(), Loader=yaml.FullLoader)

        self.vdd = self.design["vdd"]
        self.target_impedance_value = 0.1 * self.vdd * self.vdd / self.design["chiplets"][0]["power"]
        
        self.base_ckt = build_ac_ckt(ckt_file, result_file)
        self.frequency_points = np.geomspace(0.1e9, 10e9, 100) # 100ä¸ªé¢‘ç‡ç‚¹
        
        self.interposer_w = self.design["interposer"]["w"]
        self.interposer_h = self.design["interposer"]["h"]
        self.initial_candidate_branch = [
            f"cd_{x}_{y}" for x in range(self.interposer_w) for y in range(self.interposer_h)
            if (x, y) not in self.initial_result["tsvs"]
        ]
        self.num_initial_candidates = len(self.initial_candidate_branch)

        # ------------------ 2. å®šä¹‰è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´ (ä¸æ‚¨åŸç‰ˆç›¸åŒ) ------------------
        num_freq_points = len(self.frequency_points)
        # self.observation_space = spaces.Dict({
        #     "impedance": spaces.Box(low=0, high=1, shape=(num_freq_points,), dtype=np.float64),
        #     "target_z": spaces.Box(low=0, high=1, shape=(num_freq_points,), dtype=np.float64),
        #     "decap_map": spaces.Box(low=0, high=1, shape=(self.interposer_w, self.interposer_h), dtype=np.float64)
        # })

        self.observation_space = spaces.Dict({
            "impedance": spaces.Box(low=0, high=1, shape=(100,), dtype=np.float32),
            "target_z": spaces.Box(low=0, high=1, shape=(100,), dtype=np.float32),
            "decap_map": spaces.Box(low=0, high=1, shape=(self.interposer_h, self.interposer_w), dtype=np.float32),
            "context_map": spaces.Box(low=0, high=1, shape=(3, self.interposer_h, self.interposer_w), dtype=np.float32),
            "global_map": spaces.Box(low=0, high=1, shape=(3, 64, 64), dtype=np.float32),
            "local_mask": spaces.Box(low=0, high=1, shape=(1, 64, 64), dtype=np.float32),
        })

        self.action_space = spaces.Discrete(self.num_initial_candidates)
        
        # æ‰©å¤§é˜»æŠ—å€¼ï¼Œä¾¿äºå½’ä¸€åŒ–æ“ä½œ
        self.Z_MIN = 1e-4 
        self.Z_MAX = 0.3 # æˆ–è€…å¯ä»¥ç¨å¾®æ”¾å®½ä¸€ç‚¹ï¼Œæ¯”å¦‚ 0.1ï¼Œä»¥åº”å¯¹å¯èƒ½çš„å¼‚å¸¸å€¼
        self.Z_RANGE = self.Z_MAX - self.Z_MIN

        # ------------------ 3. å‡†å¤‡ä»¿çœŸæ‰€éœ€çš„é™æ€å‚æ•° (ä¸æ‚¨åŸç‰ˆç›¸åŒ) ------------------
        self._prepare_sim_tensors()
        self.render_mode = render_mode

        # ------------------ 4. åˆå§‹åŒ–å¥–åŠ±å‡½æ•°æ‰€éœ€çš„çŠ¶æ€å˜é‡ ------------------
        # åœ¨ reset() ä¸­ä¼šè¢«æ­£ç¡®èµ‹å€¼
        self.total_violation = 0.0

    # _prepare_sim_tensors, _get_impedances, _get_obs, action_masks, _get_info
    # è¿™äº›è¾…åŠ©å‡½æ•°ä¸æ‚¨çš„ç‰ˆæœ¬å®Œå…¨ç›¸åŒï¼Œè¿™é‡Œä¸ºäº†ç®€æ´çœç•¥ï¼Œç›´æ¥ä½¿ç”¨æ‚¨åŸæœ‰çš„å³å¯
    # ... (æ­¤å¤„çœç•¥æ‚¨å·²æœ‰çš„ _prepare_sim_tensors, _get_impedances, _get_obs, action_masks, _get_info å‡½æ•°)
    def _prepare_sim_tensors(self):
        observe_branch = ["id"]
        typ, u, v, val, index = self.base_ckt.prepare_sim("0", observe_branch + self.initial_candidate_branch)
        self.typ, self.u, self.v = typ, u, v
        self.g_index, self.r_index, self.l_index, self.v_index = [torch.tensor([], dtype=torch.long)] * 4
        self.c_index_map = index[len(observe_branch):]
        self.i_index = index[:len(observe_branch)]
        (self.all_exc_index,) = torch.where((typ == BranchType.I) | (typ == BranchType.V))
        self.g_value, self.r_value, self.l_value = [torch.tensor([], dtype=torch.float64)] * 3
        self.initial_c_base_value = val[self.c_index_map]
        self.all_exc_value = val[self.all_exc_index]
        self.sims = [AcSimulationCuDSS(typ, u, v, val, freq) for freq in self.frequency_points]

    def _get_impedances(self):
        zs = []
        c_value_tensor = torch.zeros_like(self.initial_c_base_value)
        if self.placed_capacitor_indices:
            placed_indices_tensor = torch.tensor(self.placed_capacitor_indices)
            c_value_tensor[placed_indices_tensor] = self.initial_c_base_value[placed_indices_tensor]
        c_index_tensor = self.c_index_map
        for freq, sim in zip(self.frequency_points, self.sims):
            sim.alter(c_index_tensor, c_value_tensor)
            sim.factorize()
            sim.solve()
            i_voltage = sim.branch_voltage(self.i_index)
            zs.append(i_voltage)
        return torch.cat(zs).abs().detach().numpy()

    # def _get_obs(self):
    #     impedances_np = self._get_impedances()
    #     decap_map = np.zeros((self.interposer_w, self.interposer_h), dtype=np.float64)
    #     if self.placed_capacitor_indices:
    #         for action_index in self.placed_capacitor_indices:
    #             info = self.initial_candidate_branch[action_index]
    #             match = re.search(r"cd_(\d+)_(\d+)", info)
    #             if match:
    #                 x, y = int(match.group(1)), int(match.group(2))
    #                 decap_map[x, y] = 1.0
    #     return {"impedance": impedances_np, "target_z": np.full_like(impedances_np, self.target_impedance_value), "decap_map": decap_map}
        
    #     # ## å½’ä¸€åŒ–æ“ä½œ
    #     # # 1. è·å–åŸå§‹çš„ã€æœªå½’ä¸€åŒ–çš„é˜»æŠ—å€¼
    #     # impedances_np_raw = self._get_impedances()
        
    #     # # 2. ã€æ–°çš„å½’ä¸€åŒ–æ–¹æ³•ã€‘ Min-Max Scaling
    #     # # å…¬å¼: (X - min) / (max - min)
    #     # impedances_scaled = (impedances_np_raw - self.Z_MIN) / self.Z_RANGE
        
    #     # # å¯¹ç›®æ ‡å€¼ä¹ŸåšåŒæ ·çš„å¤„ç†
    #     # target_z_raw = np.full_like(impedances_np_raw, self.target_impedance_value)
    #     # target_z_scaled = (target_z_raw - self.Z_MIN) / self.Z_RANGE

    #     # # 3. è£å‰ªä»¥ç¡®ä¿åœ¨ [0, 1] èŒƒå›´å†…
    #     # # è¿™ä¸€æ­¥ä»ç„¶éå¸¸é‡è¦ï¼Œå¯ä»¥å¤„ç†æ‰ä»»ä½•è¶…å‡ºä½ é¢„è®¾èŒƒå›´çš„æ„å¤–å€¼
    #     # impedances_normalized = np.clip(impedances_scaled, 0, 1)
    #     # target_z_normalized = np.clip(target_z_scaled, 0, 1)

    #     # # ... (decap_map éƒ¨åˆ†ä¸å˜) ...
    #     # decap_map = np.zeros((self.interposer_w, self.interposer_h), dtype=np.float64)
    #     # if self.placed_capacitor_indices:
    #     #     for action_index in self.placed_capacitor_indices:
    #     #         info = self.initial_candidate_branch[action_index]
    #     #         match = re.search(r"cd_(\d+)_(\d+)", info)
    #     #         if match:
    #     #             x, y = int(match.group(1)), int(match.group(2))
    #     #             decap_map[x, y] = 1.0
    #     # # 5. è¿”å›å½’ä¸€åŒ–åçš„è§‚æµ‹å­—å…¸
    #     # return {
    #     #     "impedance": impedances_normalized, 
    #     #     "target_z": target_z_normalized, 
    #     #     "decap_map": decap_map
    #     # }

    def _get_obs(self):
        # 1. è·å–é˜»æŠ—ï¼ˆé•¿åº¦ = freq ç‚¹æ•°ï¼‰
        impedances_np = self._get_impedances()
        
        # 2. æ„é€  decap_map
        decap_map = np.zeros((self.interposer_h, self.interposer_w), dtype=np.float32)
        if self.placed_capacitor_indices:
            for action_index in self.placed_capacitor_indices:
                info = self.initial_candidate_branch[action_index]
                match = re.search(r"cd_(\d+)_(\d+)", info)
                if match:
                    x, y = int(match.group(1)), int(match.group(2))
                    decap_map[y, x] = 1.0

        # 3. æ„é€  context_mapï¼ˆå½¢çŠ¶ [3, W, H]ï¼‰
        context_map = np.zeros((3, self.interposer_h, self.interposer_w), dtype=np.float32)
        for chiplet in self.design["chiplets"]:
            chip_power = chiplet["power"] / 3.0         # å½’ä¸€åŒ– powerï¼ˆæœ€å¤§ 3ï¼‰
            chip_freq = chiplet["freq"] / 10e9          # å½’ä¸€åŒ– freqï¼ˆæœ€å¤§ 10GHzï¼‰

            for x in range(chiplet["x"], chiplet["x"] + chiplet["w"]):
                for y in range(chiplet["y"], chiplet["y"] + chiplet["h"]):
                    if 0 <= x < self.interposer_w and 0 <= y < self.interposer_h:
                        context_map[0, y, x] = 1.0          # chiplet æ ‡å¿—
                        context_map[1, y, x] = chip_power
                        context_map[2, y, x] = chip_freq

        # 4. æ„é€ ç›®æ ‡é˜»æŠ—å‘é‡
        target_z = np.full_like(impedances_np, self.target_impedance_value, dtype=np.float32)
        
        global_map = np.zeros((3, 64, 64), dtype=np.float32)
        local_mask = np.zeros((1, 64, 64), dtype=np.float32)

        # å°† decap_map / context_map æ˜ å°„åˆ° global_map ä¸­ï¼ˆå·¦ä¸Šè§’å¯¹é½ï¼‰
        h, w = self.interposer_h, self.interposer_w
        global_map[:, :h, :w] = context_map

        # åŒç†æ„å»º local_maskï¼ˆä¾‹å¦‚å°†å½“å‰èŠ¯ç²’åŒºåŸŸ mask æ‰ï¼‰
        for chiplet in self.design["chiplets"]:
            for x in range(chiplet["x"], chiplet["x"] + chiplet["w"]):
                for y in range(chiplet["y"], chiplet["y"] + chiplet["h"]):
                    if x < 64 and y < 64:
                        local_mask[0, y, x] = 1.0

        # 5. è¿”å›è§‚å¯Ÿå­—å…¸ï¼ˆæ³¨æ„å…¨éƒ¨ä¸º float32ï¼‰
        return {
            "impedance": impedances_np.astype(np.float32),
            "target_z": target_z,
            "decap_map": decap_map,
            "context_map": context_map,
            "global_map": global_map,
            "local_mask": local_mask, 
        }

    def action_masks(self) -> np.ndarray:
        mask = np.ones(self.num_initial_candidates, dtype=bool)
        if self.placed_capacitor_indices:
            mask[self.placed_capacitor_indices] = False
        return mask

    def _get_info(self):
        return {"action_mask": self.action_masks()}


    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        
        # é‡ç½®çŠ¶æ€
        self.placed_capacitor_indices = []
        self.current_step = 0
        
        # è·å–åˆå§‹è§‚æµ‹
        observation = self._get_obs()
        
        # ã€ä¿®æ”¹ã€‘è®¡ç®—åˆå§‹çš„æ€»è¿è§„é‡
        impedances = observation["impedance"]
        violation_array = np.maximum(impedances - self.target_impedance_value, 0)
        self.total_violation = np.sum(violation_array)
        
        info = self._get_info()
        
        if self.render_mode == "console":
            self.render(observation, 0) # åˆå§‹å¥–åŠ±ä¸º0

        return observation, info
    
    def step(self, action):
    # 1. ã€æ ¸å¿ƒä¿®æ”¹ã€‘åœ¨æ‰§è¡ŒåŠ¨ä½œå‰ï¼Œè®°å½•æ—§çš„è¾¾æ ‡ç‚¹æ•°
        old_obs = self._get_obs()
        old_impedances = old_obs["impedance"]
        old_points_below_target = np.sum(old_impedances <= self.target_impedance_value)
        
        # 2. æ‰§è¡ŒåŠ¨ä½œ
        self.placed_capacitor_indices.append(action)
        self.current_step += 1

        # 3. è·å–æ–°çš„è§‚æµ‹å€¼
        observation = self._get_obs()
        new_impedances = observation["impedance"]
        new_points_below_target = np.sum(new_impedances <= self.target_impedance_value)

        # 4. ã€æ ¸å¿ƒä¿®æ”¹ã€‘è®¡ç®—æ–°çš„å¥–åŠ±
        # 4.1 ä¸»è¦å¥–åŠ±ï¼šåŸºäºè¾¾æ ‡ç‚¹æ•°çš„å¢åŠ é‡
        improvement_reward = (new_points_below_target - old_points_below_target)
        
        # æ”¾å¤§è¿™ä¸ªä¿¡å·ï¼Œä½¿å…¶æ¯”è¡ŒåŠ¨æˆæœ¬æ›´é‡è¦
        # ä¾‹å¦‚ï¼Œæ¯å¤šä¸€ä¸ªç‚¹è¾¾æ ‡ï¼Œå°±å¥–åŠ± 5 åˆ†
        reward = improvement_reward * 5.0
        
        # 4.2 è¡ŒåŠ¨æˆæœ¬ï¼šæ¯ä¸€æ­¥éƒ½ç»™äºˆä¸€ä¸ªå°çš„è´Ÿå¥–åŠ±
        action_cost = -2 # è¿™ä¸ªå€¼æ˜¯è¶…å‚æ•°ï¼Œå¯ä»¥è°ƒæ•´
        reward += action_cost

        # 5. åˆ¤æ–­å›åˆæ˜¯å¦ç»“æŸ
        # 5.1 æˆåŠŸç»ˆæ­¢ (Terminated): æ‰€æœ‰é¢‘ç‡ç‚¹éƒ½è¾¾æ ‡
        terminated = bool(new_points_below_target == len(self.frequency_points))
        
        # 5.2 å¤±è´¥æˆªæ–­ (Truncated): æ‰€æœ‰å¯ç”¨ä½ç½®éƒ½å·²æ”¾ç½®ç”µå®¹
        truncated = (len(self.placed_capacitor_indices) == self.num_initial_candidates)
        
        # 6. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¸ºå›åˆç»“æŸæ·»åŠ æœ€ç»ˆå¥–åŠ±æˆ–æƒ©ç½š
        if terminated:
            final_reward = 100.0
            efficiency_bonus = (self.num_initial_candidates - len(self.placed_capacitor_indices)) * 0.5
            final_reward += efficiency_bonus
            
            reward += final_reward
            print(f"ğŸ‰ğŸ‰ğŸ‰ æ”¾ç½®æˆåŠŸ! ä½¿ç”¨ç”µå®¹æ•°é‡: {len(self.placed_capacitor_indices)}, è·å¾—æ•ˆç‡å¥–åŠ±: {efficiency_bonus:.2f}")
            # reward += 200.0
            # print(f"ğŸ‰ğŸ‰ğŸ‰ æ”¾ç½®æˆåŠŸ! ä½¿ç”¨ç”µå®¹æ•°é‡: {len(self.placed_capacitor_indices)}")

        elif truncated and not terminated:
            reward -= 100 # ç”¨å°½æ‰€æœ‰ç”µå®¹ä»æœªæˆåŠŸï¼Œç»™äºˆå·¨å¤§çš„æƒ©ç½š
            print(f"ğŸ”¥ğŸ”¥ğŸ”¥ æ”¾ç½®å¤±è´¥! æ‰€æœ‰å¯ç”¨ä½ç½®å·²ç”¨å°½ã€‚")

        # 7. è·å–ä¿¡æ¯ï¼ˆåŒ…å«æ›´æ–°åçš„åŠ¨ä½œæ©ç ï¼‰
        info = self._get_info()

        if self.render_mode == "console":
            self.render(observation, reward)
            
        return observation, reward, terminated, truncated, info


    # def step(self, action):
    #     # 1. æ‰§è¡ŒåŠ¨ä½œ: å°†é€‰æ‹©çš„ç”µå®¹ä½ç½®è®°å½•ä¸‹æ¥
    #     #    æ³¨æ„ï¼šæˆ‘ä»¬ä¸å†éœ€è¦æ£€æŸ¥ action æ˜¯å¦å·²è¢«æ”¾ç½®ï¼Œå› ä¸º MaskablePPO ä¼šå¤„ç†è¿™ä¸ªé—®é¢˜ã€‚
    #     self.placed_capacitor_indices.append(action)
    #     self.current_step += 1

    #     # 2. è·å–æ–°çš„è§‚æµ‹å€¼
    #     observation = self._get_obs()
    #     new_impedances = observation["impedance"]

    #     # 3. ã€æ ¸å¿ƒä¿®æ”¹ã€‘è®¡ç®—æ–°çš„å¥–åŠ±
    #     # 3.1 è®¡ç®—æ–°çš„æ€»è¿è§„é‡
    #     new_violation_array = np.maximum(new_impedances - self.target_impedance_value, 0)
    #     new_total_violation = np.sum(new_violation_array)

    #     # 3.2 è®¡ç®—å¥–åŠ± = è¿è§„é‡çš„å‡å°‘é‡ - è¡ŒåŠ¨æˆæœ¬
    #     # è¿è§„é‡å‡å°‘å¾—è¶Šå¤šï¼Œè¿™ä¸ªå€¼å°±è¶Šå¤§ï¼Œå¥–åŠ±ä¹Ÿè¶Šé«˜
    #     violation_reduction = self.total_violation - new_total_violation
        
    #     # å°†å‡å°‘é‡æ”¾å¤§ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ›´å¼ºçš„å­¦ä¹ ä¿¡å·
    #     reward = violation_reduction * 10.0
        
    #     # å¢åŠ ä¸€ä¸ªå°çš„è´Ÿå¥–åŠ±ä½œä¸ºæ”¾ç½®ç”µå®¹çš„â€œæˆæœ¬â€ï¼Œé¼“åŠ±æ™ºèƒ½ä½“ä½¿ç”¨æ›´å°‘çš„ç”µå®¹
    #     reward -= 0.5  # è¿™ä¸ªå€¼æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå¯ä»¥è°ƒæ•´

    #     # 3.3 æ›´æ–°ç¯å¢ƒçŠ¶æ€ï¼Œä¸ºä¸‹ä¸€æ­¥åšå‡†å¤‡
    #     self.total_violation = new_total_violation

    #     # 4. åˆ¤æ–­å›åˆæ˜¯å¦ç»“æŸ
    #     # 4.1 æˆåŠŸç»ˆæ­¢ (Terminated): å½“æ€»è¿è§„é‡å°äºä¸€ä¸ªæå°å€¼æ—¶ï¼Œè®¤ä¸ºä»»åŠ¡æˆåŠŸ
    #     terminated = bool(self.total_violation < 1e-6)

    #     # 4.2 å¤±è´¥æˆªæ–­ (Truncated): å½“æ‰€æœ‰å¯ç”¨ä½ç½®éƒ½å·²æ”¾ç½®ç”µå®¹
    #     truncated = (len(self.placed_capacitor_indices) == self.num_initial_candidates)

    #     # 5. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¸ºå›åˆç»“æŸæ·»åŠ æœ€ç»ˆå¥–åŠ±æˆ–æƒ©ç½š
    #     if terminated:
    #         reward += 200  # æˆåŠŸå®Œæˆä»»åŠ¡ï¼Œç»™äºˆå·¨å¤§çš„é¢å¤–å¥–åŠ±
    #         print(f"ğŸ‰ğŸ‰ğŸ‰ æ”¾ç½®æˆåŠŸ! ä½¿ç”¨ç”µå®¹æ•°é‡: {len(self.placed_capacitor_indices)}")
            
    #     elif truncated and not terminated:
    #         reward -= 100 # ç”¨å°½æ‰€æœ‰ç”µå®¹ä»æœªæˆåŠŸï¼Œç»™äºˆå·¨å¤§çš„æƒ©ç½š
    #         print(f"ğŸ”¥ğŸ”¥ğŸ”¥ æ”¾ç½®å¤±è´¥! æ‰€æœ‰å¯ç”¨ä½ç½®å·²ç”¨å°½ã€‚")

    #     # 6. è·å–ä¿¡æ¯ï¼ˆåŒ…å«æ›´æ–°åçš„åŠ¨ä½œæ©ç ï¼‰
    #     info = self._get_info()

    #     if self.render_mode == "console":
    #         self.render(observation, reward)
            
    #     return observation, reward, terminated, truncated, info

    def render(self, observation, reward, mode='console'):
        # (ä¸æ‚¨åŸç‰ˆç›¸åŒ)
        if mode == 'console':
            impedances_part = observation["impedance"]
            peak_impedance = np.max(impedances_part)
            points_below_target = np.sum(impedances_part <= self.target_impedance_value)
            self.total_violation = np.sum(np.maximum(impedances_part - self.target_impedance_value, 0))


            print(
                f"Step: {self.current_step}, "
                f"Placed Caps: {len(self.placed_capacitor_indices)}, "
                f"Reward: {reward:+.4f}, "
                f"Total Violation: {self.total_violation:.4f}, "
                f"Peak Impedance: {peak_impedance:.4f}, "
                f"Points Below Target: {points_below_target}/{len(self.frequency_points)}"
            )
            